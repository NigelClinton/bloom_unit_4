{
  "nbformat": 4,
  "nbformat_minor": 2,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3.9.1 64-bit"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.1"
    },
    "colab": {
      "name": "LS_DS_414_Topic_Modeling_Assignment_Guide.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "interpreter": {
      "hash": "b56204e04fac71cbe4e15d7b3dab17d89d2f24d87d00adcaa585640e5d78c50d"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "<img align=\"left\" src=\"https://lever-client-logos.s3.amazonaws.com/864372b1-534c-480e-acd5-9711f850815c-1524247202159.png\" width=200>\n",
        "<br></br>\n",
        "<br></br>\n",
        "\n",
        "# Topic Modeling\n",
        "## *Data Science Unit 4 Sprint 1 Assignment 4*\n",
        "\n",
        "**Analyze a corpus of Amazon reviews from Unit 4 Sprint 1 Module 1's using topic modeling: \n",
        "\n",
        "- Load in the Amazon Review dataset\n",
        "- Clean the dataset \n",
        "- Vectorize the dataset \n",
        "- Fit a Gensim LDA topic model on Amazon Reviews\n",
        "- Select appropriate number of topics\n",
        "- Create some dope visualization of the topics\n",
        "- Write a few bullets on your findings in markdown at the end\n",
        "- **Note**: You don't *have* to use generators for this assignment"
      ],
      "metadata": {
        "id": "5utTU63WzGd3"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "# An easy way to get the dataset loaded quickly\r\n",
        "!git clone https://github.com/LambdaSchool/DS-Unit-4-Sprint-1-NLP.git"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'DS-Unit-4-Sprint-1-NLP'...\n",
            "remote: Enumerating objects: 1528, done.\u001b[K\n",
            "remote: Total 1528 (delta 0), reused 0 (delta 0), pack-reused 1528\u001b[K\n",
            "Receiving objects: 100% (1528/1528), 56.37 MiB | 17.01 MiB/s, done.\n",
            "Resolving deltas: 100% (169/169), done.\n"
          ]
        }
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c_L4bKk_4E84",
        "outputId": "508df652-2901-483d-aa84-d960789c3562"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "!pip install pyLDAvis"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pyLDAvis\n",
            "  Downloading pyLDAvis-3.3.1.tar.gz (1.7 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.7 MB 7.3 MB/s \n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Installing backend dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "    Preparing wheel metadata ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numexpr in /usr/local/lib/python3.7/dist-packages (from pyLDAvis) (2.7.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from pyLDAvis) (57.4.0)\n",
            "Requirement already satisfied: gensim in /usr/local/lib/python3.7/dist-packages (from pyLDAvis) (3.6.0)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from pyLDAvis) (0.22.2.post1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from pyLDAvis) (1.4.1)\n",
            "Collecting funcy\n",
            "\u001b[33m  WARNING: Retrying (Retry(total=4, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ProtocolError('Connection aborted.', ConnectionResetError(104, 'Connection reset by peer'))': /packages/44/52/5cf7401456a461e4b481650dfb8279bc000f31a011d0918904f86e755947/funcy-1.16-py2.py3-none-any.whl\u001b[0m\n",
            "  Downloading funcy-1.16-py2.py3-none-any.whl (32 kB)\n",
            "Collecting numpy>=1.20.0\n",
            "  Downloading numpy-1.21.2-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (15.7 MB)\n",
            "\u001b[K     |████████████████████████████████| 15.7 MB 185 kB/s \n",
            "\u001b[?25hRequirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from pyLDAvis) (0.16.0)\n",
            "Requirement already satisfied: sklearn in /usr/local/lib/python3.7/dist-packages (from pyLDAvis) (0.0)\n",
            "Collecting pandas>=1.2.0\n",
            "  Downloading pandas-1.3.2-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (11.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 11.3 MB 47.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: jinja2 in /usr/local/lib/python3.7/dist-packages (from pyLDAvis) (2.11.3)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from pyLDAvis) (1.0.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=1.2.0->pyLDAvis) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=1.2.0->pyLDAvis) (2018.9)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas>=1.2.0->pyLDAvis) (1.15.0)\n",
            "Requirement already satisfied: smart-open>=1.2.1 in /usr/local/lib/python3.7/dist-packages (from gensim->pyLDAvis) (5.1.0)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2->pyLDAvis) (2.0.1)\n",
            "Building wheels for collected packages: pyLDAvis\n",
            "  Building wheel for pyLDAvis (PEP 517) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyLDAvis: filename=pyLDAvis-3.3.1-py2.py3-none-any.whl size=136897 sha256=b6ef1888739f8512b11e9df9cc8dfe6859a45d0d8a33512bf60ffbf1465600f9\n",
            "  Stored in directory: /root/.cache/pip/wheels/c9/21/f6/17bcf2667e8a68532ba2fbf6d5c72fdf4c7f7d9abfa4852d2f\n",
            "Successfully built pyLDAvis\n",
            "Installing collected packages: numpy, pandas, funcy, pyLDAvis\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 1.19.5\n",
            "    Uninstalling numpy-1.19.5:\n",
            "      Successfully uninstalled numpy-1.19.5\n",
            "  Attempting uninstall: pandas\n",
            "    Found existing installation: pandas 1.1.5\n",
            "    Uninstalling pandas-1.1.5:\n",
            "      Successfully uninstalled pandas-1.1.5\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "tensorflow 2.6.0 requires numpy~=1.19.2, but you have numpy 1.21.2 which is incompatible.\n",
            "google-colab 1.0.0 requires pandas~=1.1.0; python_version >= \"3.0\", but you have pandas 1.3.2 which is incompatible.\n",
            "datascience 0.10.6 requires folium==0.2.1, but you have folium 0.8.3 which is incompatible.\n",
            "albumentations 0.1.12 requires imgaug<0.2.7,>=0.2.5, but you have imgaug 0.2.9 which is incompatible.\u001b[0m\n",
            "Successfully installed funcy-1.16 numpy-1.21.2 pandas-1.3.2 pyLDAvis-3.3.1\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "numpy",
                  "pandas"
                ]
              }
            }
          },
          "metadata": {}
        }
      ],
      "metadata": {
        "id": "IrHhaizu3ghr",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "9f362582-f7e6-416f-82a3-68a75b5eeef1"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "!pip install pandarallel==1.4.8"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pandarallel==1.4.8\n",
            "  Downloading pandarallel-1.4.8.tar.gz (14 kB)\n",
            "Requirement already satisfied: dill in /usr/local/lib/python3.7/dist-packages (from pandarallel==1.4.8) (0.3.4)\n",
            "Building wheels for collected packages: pandarallel\n",
            "  Building wheel for pandarallel (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pandarallel: filename=pandarallel-1.4.8-py3-none-any.whl size=16112 sha256=0281e0f6492bd5b821bca636e0d460ee6a63cb0d5e68f6079b815a91f61030ec\n",
            "  Stored in directory: /root/.cache/pip/wheels/c7/f2/4e/e40c8b9344cccf6b8a02d8d8808ba837e72b607c4be946878a\n",
            "Successfully built pandarallel\n",
            "Installing collected packages: pandarallel\n",
            "Successfully installed pandarallel-1.4.8\n"
          ]
        }
      ],
      "metadata": {
        "id": "1ZoYw_O0_0jF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ef9a8163-e76b-4c11-9597-279071ae5f78"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "!python -m spacy download en_core_web_md"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting en_core_web_md==2.2.5\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_md-2.2.5/en_core_web_md-2.2.5.tar.gz (96.4 MB)\n",
            "\u001b[K     |████████████████████████████████| 96.4 MB 1.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: spacy>=2.2.2 in /usr/local/lib/python3.7/dist-packages (from en_core_web_md==2.2.5) (2.2.4)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_md==2.2.5) (1.0.5)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_md==2.2.5) (3.0.5)\n",
            "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_md==2.2.5) (0.4.1)\n",
            "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_md==2.2.5) (1.0.5)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_md==2.2.5) (2.23.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_md==2.2.5) (4.62.0)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_md==2.2.5) (2.0.5)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_md==2.2.5) (1.21.2)\n",
            "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_md==2.2.5) (1.0.0)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_md==2.2.5) (0.8.2)\n",
            "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_md==2.2.5) (1.1.3)\n",
            "Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_md==2.2.5) (7.4.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_md==2.2.5) (57.4.0)\n",
            "Requirement already satisfied: importlib-metadata>=0.20 in /usr/local/lib/python3.7/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_md==2.2.5) (4.6.4)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_md==2.2.5) (3.5.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_md==2.2.5) (3.7.4.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_md==2.2.5) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_md==2.2.5) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_md==2.2.5) (2021.5.30)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_md==2.2.5) (3.0.4)\n",
            "Building wheels for collected packages: en-core-web-md\n",
            "  Building wheel for en-core-web-md (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for en-core-web-md: filename=en_core_web_md-2.2.5-py3-none-any.whl size=98051302 sha256=b832036083667660fc25cea4aa11d5171bcc6da15491f99a9c035921cd1bd61e\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-4yo937y8/wheels/69/c5/b8/4f1c029d89238734311b3269762ab2ee325a42da2ce8edb997\n",
            "Successfully built en-core-web-md\n",
            "Installing collected packages: en-core-web-md\n",
            "Successfully installed en-core-web-md-2.2.5\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the model via spacy.load('en_core_web_md')\n"
          ]
        }
      ],
      "metadata": {
        "id": "nUPIrjOB_-BR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5efde3e3-543d-49d1-c2b6-7a24f8669a9d"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Restart runtime!"
      ],
      "metadata": {
        "id": "isyLmOAZqYR0"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "source": [
        "import re\r\n",
        "import pandas as pd\r\n",
        "import numpy as np\r\n",
        "import matplotlib.pyplot as plt\r\n",
        "from sklearn.feature_extraction.text import CountVectorizer\r\n",
        "\r\n",
        "import spacy\r\n",
        "spacy.util.fix_random_seed(0)\r\n",
        "nlp = spacy.load(\"en_core_web_md\")\r\n",
        "\r\n",
        "import pyLDAvis\r\n",
        "import pyLDAvis.gensim_models as gensimvis\r\n",
        "\r\n",
        "import gensim\r\n",
        "import gensim.corpora as corpora\r\n",
        "from gensim.utils import simple_preprocess\r\n",
        "from gensim.models import CoherenceModel\r\n",
        "\r\n",
        "import pandarallel\r\n",
        "\r\n",
        "%matplotlib inline\r\n",
        "\r\n",
        "import warnings\r\n",
        "warnings.filterwarnings(\"ignore\")"
      ],
      "outputs": [],
      "metadata": {
        "id": "rXrXC0mrzGd5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d60adf07-8dfa-452c-fc4f-4f5288f52d95"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "----\n",
        "### Load the Amazon Review corpus \n",
        "This dataset is located in the Sprint 1 Module 1 directory. \n",
        "\n",
        "If the provided relative path doesn't work for you, then you'll have to provide the file path so pandas can read in the file."
      ],
      "metadata": {
        "id": "x9ph4ZzpzGd6"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "source": [
        "data_path = \"Datafiniti_Amazon_Consumer_Reviews_of_Amazon_Products_May19.csv\"\r\n",
        "df = pd.read_csv(data_path)"
      ],
      "outputs": [],
      "metadata": {
        "id": "Ov482n4pzGd6"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "df.head(2)"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>dateAdded</th>\n",
              "      <th>dateUpdated</th>\n",
              "      <th>name</th>\n",
              "      <th>asins</th>\n",
              "      <th>brand</th>\n",
              "      <th>categories</th>\n",
              "      <th>primaryCategories</th>\n",
              "      <th>imageURLs</th>\n",
              "      <th>keys</th>\n",
              "      <th>manufacturer</th>\n",
              "      <th>manufacturerNumber</th>\n",
              "      <th>reviews.date</th>\n",
              "      <th>reviews.dateSeen</th>\n",
              "      <th>reviews.didPurchase</th>\n",
              "      <th>reviews.doRecommend</th>\n",
              "      <th>reviews.id</th>\n",
              "      <th>reviews.numHelpful</th>\n",
              "      <th>reviews.rating</th>\n",
              "      <th>reviews.sourceURLs</th>\n",
              "      <th>reviews.text</th>\n",
              "      <th>reviews.title</th>\n",
              "      <th>reviews.username</th>\n",
              "      <th>sourceURLs</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>AVpgNzjwLJeJML43Kpxn</td>\n",
              "      <td>2015-10-30T08:59:32Z</td>\n",
              "      <td>2019-04-25T09:08:16Z</td>\n",
              "      <td>AmazonBasics AAA Performance Alkaline Batterie...</td>\n",
              "      <td>B00QWO9P0O,B00LH3DMUO</td>\n",
              "      <td>Amazonbasics</td>\n",
              "      <td>AA,AAA,Health,Electronics,Health &amp; Household,C...</td>\n",
              "      <td>Health &amp; Beauty</td>\n",
              "      <td>https://images-na.ssl-images-amazon.com/images...</td>\n",
              "      <td>amazonbasics/hl002619,amazonbasicsaaaperforman...</td>\n",
              "      <td>AmazonBasics</td>\n",
              "      <td>HL-002619</td>\n",
              "      <td>2017-03-02T00:00:00.000Z</td>\n",
              "      <td>2017-08-28T00:00:00Z</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>3</td>\n",
              "      <td>https://www.amazon.com/product-reviews/B00QWO9...</td>\n",
              "      <td>I order 3 of them and one of the item is bad q...</td>\n",
              "      <td>... 3 of them and one of the item is bad quali...</td>\n",
              "      <td>Byger yang</td>\n",
              "      <td>https://www.barcodable.com/upc/841710106442,ht...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>AVpgNzjwLJeJML43Kpxn</td>\n",
              "      <td>2015-10-30T08:59:32Z</td>\n",
              "      <td>2019-04-25T09:08:16Z</td>\n",
              "      <td>AmazonBasics AAA Performance Alkaline Batterie...</td>\n",
              "      <td>B00QWO9P0O,B00LH3DMUO</td>\n",
              "      <td>Amazonbasics</td>\n",
              "      <td>AA,AAA,Health,Electronics,Health &amp; Household,C...</td>\n",
              "      <td>Health &amp; Beauty</td>\n",
              "      <td>https://images-na.ssl-images-amazon.com/images...</td>\n",
              "      <td>amazonbasics/hl002619,amazonbasicsaaaperforman...</td>\n",
              "      <td>AmazonBasics</td>\n",
              "      <td>HL-002619</td>\n",
              "      <td>2016-08-31T00:00:00.000Z</td>\n",
              "      <td>2017-08-28T00:00:00Z</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>4</td>\n",
              "      <td>https://www.amazon.com/product-reviews/B00QWO9...</td>\n",
              "      <td>Bulk is always the less expensive way to go fo...</td>\n",
              "      <td>... always the less expensive way to go for pr...</td>\n",
              "      <td>ByMG</td>\n",
              "      <td>https://www.barcodable.com/upc/841710106442,ht...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                     id  ...                                         sourceURLs\n",
              "0  AVpgNzjwLJeJML43Kpxn  ...  https://www.barcodable.com/upc/841710106442,ht...\n",
              "1  AVpgNzjwLJeJML43Kpxn  ...  https://www.barcodable.com/upc/841710106442,ht...\n",
              "\n",
              "[2 rows x 24 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ],
      "metadata": {
        "id": "-yZZEwZNzGd7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 271
        },
        "outputId": "8e9b4d24-867c-4043-9f78-4df1548748be"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "----\n",
        "\n",
        "### Clean data\n",
        "\n",
        "- Create a function called `clean_data` that uses regex expressions to clean your data in preparation for the vectorizer. \n",
        "\n",
        "- Save the clean text data to a column in your dataframe named `clean_text`\n",
        "\n",
        "- Feel free to re-use old code that you have written in previous modules  "
      ],
      "metadata": {
        "id": "kdwr6x8PzGd7"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "source": [
        "df.shape"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(28332, 24)"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g5cL1Amq8iT_",
        "outputId": "5e1e4f93-dccc-4f41-fea4-03c0bd421965"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "source": [
        "df['reviews.text'][8505]"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Have had zero issues with longevity or voltage. As good as the major brand batteries'"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "dgW6kglx8T_l",
        "outputId": "cd6a2c7d-62e6-46c6-b999-57c07438db84"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "source": [
        "def clean_data(text):\r\n",
        "    \"\"\"\r\n",
        "    Accepts a single text document and performs several regex substitutions in order to clean the document. \r\n",
        "    \r\n",
        "    Parameters\r\n",
        "    ----------\r\n",
        "    text: string or object \r\n",
        "    \r\n",
        "    Returns\r\n",
        "    -------\r\n",
        "    text: string or object\r\n",
        "    \"\"\"\r\n",
        "    \r\n",
        "    # order of operations - apply the expression from top to bottom\r\n",
        "    email_regex = \"From: \\S*@\\S*\\s?\"\r\n",
        "    non_alpha = '[^a-zA-Z]'\r\n",
        "    multi_white_spaces = \"[ ]{2,}\"\r\n",
        "    \r\n",
        "    text = re.sub(email_regex, \"\", text)\r\n",
        "    text = re.sub(non_alpha, ' ', text)\r\n",
        "    text = re.sub(multi_white_spaces, \" \", text)\r\n",
        "    \r\n",
        "    # apply case normalization \r\n",
        "    return text.lower().lstrip().rstrip()\r\n"
      ],
      "outputs": [],
      "metadata": {
        "deletable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "a2513c0ebd04cf8645fe0a1feb1e1a36",
          "grade": false,
          "grade_id": "cell-fa0950cfe5ef7725",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false
        },
        "id": "4xo_hkHKzGd7"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "source": [
        "df.head(2)"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>dateAdded</th>\n",
              "      <th>dateUpdated</th>\n",
              "      <th>name</th>\n",
              "      <th>asins</th>\n",
              "      <th>brand</th>\n",
              "      <th>categories</th>\n",
              "      <th>primaryCategories</th>\n",
              "      <th>imageURLs</th>\n",
              "      <th>keys</th>\n",
              "      <th>...</th>\n",
              "      <th>reviews.didPurchase</th>\n",
              "      <th>reviews.doRecommend</th>\n",
              "      <th>reviews.id</th>\n",
              "      <th>reviews.numHelpful</th>\n",
              "      <th>reviews.rating</th>\n",
              "      <th>reviews.sourceURLs</th>\n",
              "      <th>reviews.text</th>\n",
              "      <th>reviews.title</th>\n",
              "      <th>reviews.username</th>\n",
              "      <th>sourceURLs</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>AVpgNzjwLJeJML43Kpxn</td>\n",
              "      <td>2015-10-30T08:59:32Z</td>\n",
              "      <td>2019-04-25T09:08:16Z</td>\n",
              "      <td>AmazonBasics AAA Performance Alkaline Batterie...</td>\n",
              "      <td>B00QWO9P0O,B00LH3DMUO</td>\n",
              "      <td>Amazonbasics</td>\n",
              "      <td>AA,AAA,Health,Electronics,Health &amp; Household,C...</td>\n",
              "      <td>Health &amp; Beauty</td>\n",
              "      <td>https://images-na.ssl-images-amazon.com/images...</td>\n",
              "      <td>amazonbasics/hl002619,amazonbasicsaaaperforman...</td>\n",
              "      <td>...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>3</td>\n",
              "      <td>https://www.amazon.com/product-reviews/B00QWO9...</td>\n",
              "      <td>I order 3 of them and one of the item is bad q...</td>\n",
              "      <td>... 3 of them and one of the item is bad quali...</td>\n",
              "      <td>Byger yang</td>\n",
              "      <td>https://www.barcodable.com/upc/841710106442,ht...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>AVpgNzjwLJeJML43Kpxn</td>\n",
              "      <td>2015-10-30T08:59:32Z</td>\n",
              "      <td>2019-04-25T09:08:16Z</td>\n",
              "      <td>AmazonBasics AAA Performance Alkaline Batterie...</td>\n",
              "      <td>B00QWO9P0O,B00LH3DMUO</td>\n",
              "      <td>Amazonbasics</td>\n",
              "      <td>AA,AAA,Health,Electronics,Health &amp; Household,C...</td>\n",
              "      <td>Health &amp; Beauty</td>\n",
              "      <td>https://images-na.ssl-images-amazon.com/images...</td>\n",
              "      <td>amazonbasics/hl002619,amazonbasicsaaaperforman...</td>\n",
              "      <td>...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>4</td>\n",
              "      <td>https://www.amazon.com/product-reviews/B00QWO9...</td>\n",
              "      <td>Bulk is always the less expensive way to go fo...</td>\n",
              "      <td>... always the less expensive way to go for pr...</td>\n",
              "      <td>ByMG</td>\n",
              "      <td>https://www.barcodable.com/upc/841710106442,ht...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>2 rows × 24 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                     id             dateAdded           dateUpdated  \\\n",
              "0  AVpgNzjwLJeJML43Kpxn  2015-10-30T08:59:32Z  2019-04-25T09:08:16Z   \n",
              "1  AVpgNzjwLJeJML43Kpxn  2015-10-30T08:59:32Z  2019-04-25T09:08:16Z   \n",
              "\n",
              "                                                name                  asins  \\\n",
              "0  AmazonBasics AAA Performance Alkaline Batterie...  B00QWO9P0O,B00LH3DMUO   \n",
              "1  AmazonBasics AAA Performance Alkaline Batterie...  B00QWO9P0O,B00LH3DMUO   \n",
              "\n",
              "          brand                                         categories  \\\n",
              "0  Amazonbasics  AA,AAA,Health,Electronics,Health & Household,C...   \n",
              "1  Amazonbasics  AA,AAA,Health,Electronics,Health & Household,C...   \n",
              "\n",
              "  primaryCategories                                          imageURLs  \\\n",
              "0   Health & Beauty  https://images-na.ssl-images-amazon.com/images...   \n",
              "1   Health & Beauty  https://images-na.ssl-images-amazon.com/images...   \n",
              "\n",
              "                                                keys  ... reviews.didPurchase  \\\n",
              "0  amazonbasics/hl002619,amazonbasicsaaaperforman...  ...                 NaN   \n",
              "1  amazonbasics/hl002619,amazonbasicsaaaperforman...  ...                 NaN   \n",
              "\n",
              "  reviews.doRecommend reviews.id reviews.numHelpful reviews.rating  \\\n",
              "0                 NaN        NaN                NaN              3   \n",
              "1                 NaN        NaN                NaN              4   \n",
              "\n",
              "                                  reviews.sourceURLs  \\\n",
              "0  https://www.amazon.com/product-reviews/B00QWO9...   \n",
              "1  https://www.amazon.com/product-reviews/B00QWO9...   \n",
              "\n",
              "                                        reviews.text  \\\n",
              "0  I order 3 of them and one of the item is bad q...   \n",
              "1  Bulk is always the less expensive way to go fo...   \n",
              "\n",
              "                                       reviews.title  reviews.username  \\\n",
              "0  ... 3 of them and one of the item is bad quali...        Byger yang   \n",
              "1  ... always the less expensive way to go for pr...              ByMG   \n",
              "\n",
              "                                          sourceURLs  \n",
              "0  https://www.barcodable.com/upc/841710106442,ht...  \n",
              "1  https://www.barcodable.com/upc/841710106442,ht...  \n",
              "\n",
              "[2 rows x 24 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "source": [
        "df[\"clean_data\"] = df[\"content\"].apply(clean_data)"
      ],
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "expected string or bytes-like object",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_9364/613942381.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# create a clean_text column by applying  clean_data to your text\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"clean_text\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mclean_data\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36mapply\u001b[1;34m(self, func, axis, raw, result_type, args, **kwargs)\u001b[0m\n\u001b[0;32m   8734\u001b[0m             \u001b[0mkwargs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   8735\u001b[0m         )\n\u001b[1;32m-> 8736\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mop\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   8737\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   8738\u001b[0m     def applymap(\n",
            "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pandas\\core\\apply.py\u001b[0m in \u001b[0;36mapply\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    686\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply_raw\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    687\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 688\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply_standard\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    689\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    690\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0magg\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pandas\\core\\apply.py\u001b[0m in \u001b[0;36mapply_standard\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    803\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    804\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mapply_standard\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 805\u001b[1;33m         \u001b[0mresults\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mres_index\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply_series_generator\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    806\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    807\u001b[0m         \u001b[1;31m# wrap results\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pandas\\core\\apply.py\u001b[0m in \u001b[0;36mapply_series_generator\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    819\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mv\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mseries_gen\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    820\u001b[0m                 \u001b[1;31m# ignore SettingWithCopy here in case the user mutates\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 821\u001b[1;33m                 \u001b[0mresults\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mv\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    822\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresults\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mABCSeries\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    823\u001b[0m                     \u001b[1;31m# If we have a view on v, we need to make a copy because\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_9364/961986823.py\u001b[0m in \u001b[0;36mclean_data\u001b[1;34m(text)\u001b[0m\n\u001b[0;32m     17\u001b[0m     \u001b[0mmulti_white_spaces\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"[ ]{2,}\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 19\u001b[1;33m     \u001b[0mtext\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mre\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msub\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0memail_regex\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     20\u001b[0m     \u001b[0mtext\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mre\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msub\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnon_alpha\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m' '\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m     \u001b[0mtext\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mre\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msub\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmulti_white_spaces\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\" \"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\re.py\u001b[0m in \u001b[0;36msub\u001b[1;34m(pattern, repl, string, count, flags)\u001b[0m\n\u001b[0;32m    208\u001b[0m     \u001b[0ma\u001b[0m \u001b[0mcallable\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mit\u001b[0m\u001b[0;31m'\u001b[0m\u001b[0ms\u001b[0m \u001b[0mpassed\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mMatch\u001b[0m \u001b[0mobject\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mmust\u001b[0m \u001b[1;32mreturn\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    209\u001b[0m     a replacement string to be used.\"\"\"\n\u001b[1;32m--> 210\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0m_compile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpattern\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msub\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrepl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstring\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcount\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    211\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    212\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0msubn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpattern\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrepl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstring\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcount\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;31mTypeError\u001b[0m: expected string or bytes-like object"
          ]
        }
      ],
      "metadata": {
        "deletable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "a6ceefe5476e71efcb3d36c3e203616f",
          "grade": false,
          "grade_id": "cell-5b8e2bc0f9a745f3",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false
        },
        "id": "H693L3fJzGd8"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "alphebetical_chars = [\"ABCDEFGHIJKLMNOP\"]\r\n",
        "# check if any of these alphabetical chars exist in your clean chars\r\n",
        "assert df.clean_text.isin(alphebetical_chars).sum() == 0, \"Did you case normalize your text inside of your clean_data function?\""
      ],
      "outputs": [],
      "metadata": {
        "id": "5FMvAhkBzGd8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "------\n",
        "\n",
        "## Determine number of topics\n",
        "\n",
        "We are going to run an experiment to determine how many topics exists within the `primaryCategories` of `Electronics`. This is the largest primary category containing nearly 14K documents, so we should have plenty of data. \n",
        "\n",
        "Just as we did in the guided project, we'll be running a gridseach over the number of topics and scoring each model using the Coherence metric to determine which number of topics we should use. \n"
      ],
      "metadata": {
        "id": "kzqt1ainzGd8"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "# create a mask for docs that are in the Electronics primaryCategories - save result to `electronics_mask`\r\n",
        "electronics_mask = df.primaryCategories.isin([\"Electronics\"])\r\n",
        "\r\n",
        "# use mask to select all the documents in the Electronics primaryCategories - save result to `df_electronics`\r\n",
        "df_electronics = df[electronics_mask]"
      ],
      "outputs": [],
      "metadata": {
        "id": "3jrHewGbzGd9"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "df_electronics.head(3)"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>dateAdded</th>\n",
              "      <th>dateUpdated</th>\n",
              "      <th>name</th>\n",
              "      <th>asins</th>\n",
              "      <th>brand</th>\n",
              "      <th>categories</th>\n",
              "      <th>primaryCategories</th>\n",
              "      <th>imageURLs</th>\n",
              "      <th>keys</th>\n",
              "      <th>manufacturer</th>\n",
              "      <th>manufacturerNumber</th>\n",
              "      <th>reviews.date</th>\n",
              "      <th>reviews.dateSeen</th>\n",
              "      <th>reviews.didPurchase</th>\n",
              "      <th>reviews.doRecommend</th>\n",
              "      <th>reviews.id</th>\n",
              "      <th>reviews.numHelpful</th>\n",
              "      <th>reviews.rating</th>\n",
              "      <th>reviews.sourceURLs</th>\n",
              "      <th>reviews.text</th>\n",
              "      <th>reviews.title</th>\n",
              "      <th>reviews.username</th>\n",
              "      <th>sourceURLs</th>\n",
              "      <th>clean_text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>8343</th>\n",
              "      <td>AVpe7nGV1cnluZ0-aG2o</td>\n",
              "      <td>2014-10-28T11:14:38Z</td>\n",
              "      <td>2019-04-25T09:05:28Z</td>\n",
              "      <td>AmazonBasics Nylon CD/DVD Binder (400 Capacity)</td>\n",
              "      <td>B00DIHVMEA,B00EZ1ZTV0</td>\n",
              "      <td>Amazonbasics</td>\n",
              "      <td>Audio &amp; Video Accessories,TV, Video &amp; Home Aud...</td>\n",
              "      <td>Electronics</td>\n",
              "      <td>http://ecx.images-amazon.com/images/I/41jQha7Z...</td>\n",
              "      <td>amazonbasicsnyloncddvdbinder400capacity/b00ez1...</td>\n",
              "      <td>AmazonBasics</td>\n",
              "      <td>YBB12400R2</td>\n",
              "      <td>2017-11-12T00:00:00.000Z</td>\n",
              "      <td>2019-03-25T00:00:00Z</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>5</td>\n",
              "      <td>https://www.ebay.com/itm/Amazonbasics-Nylon-Cd...</td>\n",
              "      <td>Great case to keep everything in its place! My...</td>\n",
              "      <td>Excellent product</td>\n",
              "      <td>qs341_5</td>\n",
              "      <td>https://www.ebay.com/itm/AmazonBasics-Nylon-CD...</td>\n",
              "      <td>great case to keep everything in its place my ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8344</th>\n",
              "      <td>AVpe7nGV1cnluZ0-aG2o</td>\n",
              "      <td>2014-10-28T11:14:38Z</td>\n",
              "      <td>2019-04-25T09:05:28Z</td>\n",
              "      <td>AmazonBasics Nylon CD/DVD Binder (400 Capacity)</td>\n",
              "      <td>B00DIHVMEA,B00EZ1ZTV0</td>\n",
              "      <td>Amazonbasics</td>\n",
              "      <td>Audio &amp; Video Accessories,TV, Video &amp; Home Aud...</td>\n",
              "      <td>Electronics</td>\n",
              "      <td>http://ecx.images-amazon.com/images/I/41jQha7Z...</td>\n",
              "      <td>amazonbasicsnyloncddvdbinder400capacity/b00ez1...</td>\n",
              "      <td>AmazonBasics</td>\n",
              "      <td>YBB12400R2</td>\n",
              "      <td>2014-06-14T05:00:00Z</td>\n",
              "      <td>2014-08-28T00:00:00Z</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>5</td>\n",
              "      <td>http://www.amazon.co.uk/gp/product-reviews/B00...</td>\n",
              "      <td>After discarding and getting rid of broken cd ...</td>\n",
              "      <td>It was a much needed storage</td>\n",
              "      <td>Diablita</td>\n",
              "      <td>https://www.ebay.com/itm/AmazonBasics-Nylon-CD...</td>\n",
              "      <td>after discarding and getting rid of broken cd ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8345</th>\n",
              "      <td>AVpe7nGV1cnluZ0-aG2o</td>\n",
              "      <td>2014-10-28T11:14:38Z</td>\n",
              "      <td>2019-04-25T09:05:28Z</td>\n",
              "      <td>AmazonBasics Nylon CD/DVD Binder (400 Capacity)</td>\n",
              "      <td>B00DIHVMEA,B00EZ1ZTV0</td>\n",
              "      <td>Amazonbasics</td>\n",
              "      <td>Audio &amp; Video Accessories,TV, Video &amp; Home Aud...</td>\n",
              "      <td>Electronics</td>\n",
              "      <td>http://ecx.images-amazon.com/images/I/41jQha7Z...</td>\n",
              "      <td>amazonbasicsnyloncddvdbinder400capacity/b00ez1...</td>\n",
              "      <td>AmazonBasics</td>\n",
              "      <td>YBB12400R2</td>\n",
              "      <td>2019-02-15T00:00:00.000Z</td>\n",
              "      <td>2019-03-25T00:00:00Z</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>5</td>\n",
              "      <td>https://www.ebay.com/itm/Amazonbasics-Nylon-Cd...</td>\n",
              "      <td>A few dollars more, but I am boycotting amazon</td>\n",
              "      <td>it was worth it</td>\n",
              "      <td>coldbloodblazing</td>\n",
              "      <td>https://www.ebay.com/itm/AmazonBasics-Nylon-CD...</td>\n",
              "      <td>a few dollars more but i am boycotting amazon</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                        id  ...                                         clean_text\n",
              "8343  AVpe7nGV1cnluZ0-aG2o  ...  great case to keep everything in its place my ...\n",
              "8344  AVpe7nGV1cnluZ0-aG2o  ...  after discarding and getting rid of broken cd ...\n",
              "8345  AVpe7nGV1cnluZ0-aG2o  ...      a few dollars more but i am boycotting amazon\n",
              "\n",
              "[3 rows x 25 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 372
        },
        "id": "peb18zW3zGd9",
        "outputId": "a3978c32-5d74-4361-e642-76dc927f65e9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "------\n",
        "### Tokenize your documents \n",
        "\n",
        "Remember that you'll need to use the [**corpora**](https://radimrehurek.com/gensim/corpora/dictionary.html) class from the Gensim library. So definitely check out the docs to learn more about this tool. There is an example on how to do this in the guided project.\n",
        "\n",
        "But before we can use the [**corpora**](https://radimrehurek.com/gensim/corpora/dictionary.html) class, we must first tokenize our articles. \n",
        "\n"
      ],
      "metadata": {
        "id": "FfZWF0I9zGd-"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "# identify how many processors your machine has - save the result to `n_processors`\r\n",
        "\r\n",
        "# subtract 1 from n_processors - save the result to `nb_workers`\r\n",
        "\r\n",
        "# initialize just like we did in the guided project\r\n",
        "# YOUR CODE HERE\r\n",
        "raise NotImplementedError()"
      ],
      "outputs": [],
      "metadata": {
        "id": "2rAMfVfB1PS-"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "# load in the spaCy language model\r\n",
        "nlp = spacy.load(\"en_core_web_md\")"
      ],
      "outputs": [],
      "metadata": {
        "id": "t6pNnk3vzGd-"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "%%time\r\n",
        "# create our tokens in the form of lemmas \r\n",
        "df_electronics['lemmas'] = df_electronics['clean_text'].parallel_apply(lambda x: [token.lemma_ for token in nlp(x) if (token.is_stop != True) and (token.is_punct != True)])"
      ],
      "outputs": [],
      "metadata": {
        "id": "05x-KJPIBQa2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Use the corpora class to prep your data for LDA\n",
        "\n",
        "You'll need to create the same `id2word` and `corpus` objects that we created in the guided projects. So be sure to reference the guided project notebook if you need to. "
      ],
      "metadata": {
        "id": "WS7vTVqnzGd_"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "# Create lemma dictionary using Dictionary - save result to `id2word`\r\n",
        "id2word = \r\n",
        "\r\n",
        "# Create Term Document Frequency list - save result to `corpus`\r\n",
        "corpus = \r\n",
        "\r\n",
        "# YOUR CODE HERE\r\n",
        "raise NotImplementedError()"
      ],
      "outputs": [],
      "metadata": {
        "deletable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "53bf811aebb39e7ed6fc592851f82f8b",
          "grade": false,
          "grade_id": "cell-9d92f28649aa999e",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false
        },
        "id": "7paL9zS7zGd_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Gridsearch the number of topics \n",
        "\n",
        "Just as we did in the guided project, we're going to run a for loop over a range of possible number of topics and then plot the coherence values to determine which number of topics leads to the most sensible grouping of documents. "
      ],
      "metadata": {
        "id": "vtmHceiazGd_"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "def compute_coherence_values(dictionary, corpus, texts, limit, start=2, step=3):\r\n",
        "    \"\"\"\r\n",
        "    Compute c_v coherence for various number of topics\r\n",
        "\r\n",
        "    Parameters:\r\n",
        "    ----------\r\n",
        "    dictionary : Gensim dictionary\r\n",
        "    corpus : Gensim corpus\r\n",
        "    texts : List of input texts\r\n",
        "    limit : Max num of topics\r\n",
        "\r\n",
        "    Returns:\r\n",
        "    -------\r\n",
        "    model_list : List of LDA topic models\r\n",
        "    coherence_values : Coherence values corresponding to the LDA model with respective number of topics\r\n",
        "    \"\"\"\r\n",
        "    coherence_values = []\r\n",
        "    model_list = []\r\n",
        "    for num_topics in range(start, limit, step):\r\n",
        "        model = gensim.models.ldamulticore.LdaMulticore(corpus=corpus,\r\n",
        "                                                        id2word=id2word,\r\n",
        "                                                        num_topics=num_topics, \r\n",
        "                                                        chunksize=100,\r\n",
        "                                                        passes=10,\r\n",
        "                                                        random_state=1234,\r\n",
        "                                                        per_word_topics=True,\r\n",
        "                                                        workers=2)\r\n",
        "        model_list.append(model)\r\n",
        "        coherencemodel = CoherenceModel(model=model, texts=texts, dictionary=dictionary, coherence='c_v')\r\n",
        "        coherence_values.append(coherencemodel.get_coherence())\r\n",
        "\r\n",
        "    return model_list, coherence_values"
      ],
      "outputs": [],
      "metadata": {
        "id": "FBICFYD4zGeA"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "%%time\r\n",
        "model_list, coherence_values = compute_coherence_values(dictionary=id2word, corpus=corpus, texts=df_electronics['lemmas'], start=2, limit=16, step=2)"
      ],
      "outputs": [],
      "metadata": {
        "id": "Nn20KdDl2HUR"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "start=2; limit=16;  step=2;\r\n",
        "x = range(start, limit, step)\r\n",
        "\r\n",
        "plt.figure(figsize=(20,5))\r\n",
        "plt.grid()\r\n",
        "plt.title(\"Coherence Score vs. Number of Topics\")\r\n",
        "plt.xticks(x)\r\n",
        "plt.plot(x, coherence_values, \"-o\")\r\n",
        "\r\n",
        "plt.xlabel(\"Num Topics\")\r\n",
        "plt.ylabel(\"Coherence score\")\r\n",
        "\r\n",
        "plt.show();"
      ],
      "outputs": [],
      "metadata": {
        "id": "k2kJsVLt2P5-"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "# use np.argmax() to get index of largest coherence value from coherence_values - save result to `max_cohereance_val_index`\r\n",
        "\r\n",
        "# use `max_cohereance_val_index` to index model_list for the corresponding model - save result to `lda_trained_model`\r\n",
        "\r\n",
        "max_coherence_val_index = \r\n",
        "\r\n",
        "lda_trained_model = \r\n",
        "\r\n",
        "# YOUR CODE HERE\r\n",
        "raise NotImplementedError()"
      ],
      "outputs": [],
      "metadata": {
        "deletable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "45f72ff929ab8b765d0f0b5f35137f0d",
          "grade": false,
          "grade_id": "cell-e97661faebf1ac3c",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false
        },
        "id": "1Qx7OVX-zGeB"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "lda_trained_model"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<gensim.models.ldamulticore.LdaMulticore at 0x7f8706e798d0>"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7y-wokYcIg9_",
        "outputId": "b5fabbe1-26f3-4770-a011-f892bc76a3c5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Use pyLDAvis to visualize your topics \n",
        "\n",
        "Take a look at the topic bubbles and bar char for the terms on the right hand side.  \n",
        "\n",
        "- Describe the topic bubbles. \n",
        "- Do they overlap or not? \n",
        "- What does it mean when they overlap? \n",
        "- What does it mean when they don't overlap?\n",
        "- Are the terms in each topic distinct from the topics in the other topic bubbles?\n"
      ],
      "metadata": {
        "id": "LwGY4o8fzGeB"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "!pip install ipywidgets"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: ipywidgets in /usr/local/lib/python3.7/dist-packages (7.6.3)\n",
            "Requirement already satisfied: traitlets>=4.3.1 in /usr/local/lib/python3.7/dist-packages (from ipywidgets) (5.0.5)\n",
            "Requirement already satisfied: ipykernel>=4.5.1 in /usr/local/lib/python3.7/dist-packages (from ipywidgets) (4.10.1)\n",
            "Requirement already satisfied: jupyterlab-widgets>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from ipywidgets) (1.0.0)\n",
            "Requirement already satisfied: nbformat>=4.2.0 in /usr/local/lib/python3.7/dist-packages (from ipywidgets) (5.1.3)\n",
            "Requirement already satisfied: ipython>=4.0.0 in /usr/local/lib/python3.7/dist-packages (from ipywidgets) (5.5.0)\n",
            "Requirement already satisfied: widgetsnbextension~=3.5.0 in /usr/local/lib/python3.7/dist-packages (from ipywidgets) (3.5.1)\n",
            "Requirement already satisfied: jupyter-client in /usr/local/lib/python3.7/dist-packages (from ipykernel>=4.5.1->ipywidgets) (5.3.5)\n",
            "Requirement already satisfied: tornado>=4.0 in /usr/local/lib/python3.7/dist-packages (from ipykernel>=4.5.1->ipywidgets) (5.1.1)\n",
            "Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.7/dist-packages (from ipython>=4.0.0->ipywidgets) (57.4.0)\n",
            "Requirement already satisfied: simplegeneric>0.8 in /usr/local/lib/python3.7/dist-packages (from ipython>=4.0.0->ipywidgets) (0.8.1)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.7/dist-packages (from ipython>=4.0.0->ipywidgets) (0.7.5)\n",
            "Requirement already satisfied: prompt-toolkit<2.0.0,>=1.0.4 in /usr/local/lib/python3.7/dist-packages (from ipython>=4.0.0->ipywidgets) (1.0.18)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.7/dist-packages (from ipython>=4.0.0->ipywidgets) (4.4.2)\n",
            "Requirement already satisfied: pexpect in /usr/local/lib/python3.7/dist-packages (from ipython>=4.0.0->ipywidgets) (4.8.0)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.7/dist-packages (from ipython>=4.0.0->ipywidgets) (2.6.1)\n",
            "Requirement already satisfied: jsonschema!=2.5.0,>=2.4 in /usr/local/lib/python3.7/dist-packages (from nbformat>=4.2.0->ipywidgets) (2.6.0)\n",
            "Requirement already satisfied: ipython-genutils in /usr/local/lib/python3.7/dist-packages (from nbformat>=4.2.0->ipywidgets) (0.2.0)\n",
            "Requirement already satisfied: jupyter-core in /usr/local/lib/python3.7/dist-packages (from nbformat>=4.2.0->ipywidgets) (4.7.1)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.7/dist-packages (from prompt-toolkit<2.0.0,>=1.0.4->ipython>=4.0.0->ipywidgets) (1.15.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.7/dist-packages (from prompt-toolkit<2.0.0,>=1.0.4->ipython>=4.0.0->ipywidgets) (0.2.5)\n",
            "Requirement already satisfied: notebook>=4.4.1 in /usr/local/lib/python3.7/dist-packages (from widgetsnbextension~=3.5.0->ipywidgets) (5.3.1)\n",
            "Requirement already satisfied: nbconvert in /usr/local/lib/python3.7/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets) (5.6.1)\n",
            "Requirement already satisfied: Send2Trash in /usr/local/lib/python3.7/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets) (1.8.0)\n",
            "Requirement already satisfied: terminado>=0.8.1 in /usr/local/lib/python3.7/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets) (0.11.0)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.7/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets) (2.11.3)\n",
            "Requirement already satisfied: pyzmq>=13 in /usr/local/lib/python3.7/dist-packages (from jupyter-client->ipykernel>=4.5.1->ipywidgets) (22.2.1)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from jupyter-client->ipykernel>=4.5.1->ipywidgets) (2.8.2)\n",
            "Requirement already satisfied: ptyprocess in /usr/local/lib/python3.7/dist-packages (from terminado>=0.8.1->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets) (0.7.0)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets) (2.0.1)\n",
            "Requirement already satisfied: pandocfilters>=1.4.1 in /usr/local/lib/python3.7/dist-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets) (1.4.3)\n",
            "Requirement already satisfied: defusedxml in /usr/local/lib/python3.7/dist-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets) (0.7.1)\n",
            "Requirement already satisfied: mistune<2,>=0.8.1 in /usr/local/lib/python3.7/dist-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets) (0.8.4)\n",
            "Requirement already satisfied: bleach in /usr/local/lib/python3.7/dist-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets) (4.0.0)\n",
            "Requirement already satisfied: entrypoints>=0.2.2 in /usr/local/lib/python3.7/dist-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets) (0.3)\n",
            "Requirement already satisfied: testpath in /usr/local/lib/python3.7/dist-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets) (0.5.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from bleach->nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets) (21.0)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.7/dist-packages (from bleach->nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets) (0.5.1)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->bleach->nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets) (2.4.7)\n"
          ]
        }
      ],
      "metadata": {
        "id": "9poWZ8WgItYS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "26627697-0e17-468f-e3c8-71e543e2d5f7"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "# plot your topics here -- using pyLDAvis\n",
        "# YOUR CODE HERE\n",
        "raise NotImplementedError()"
      ],
      "outputs": [],
      "metadata": {
        "id": "YBqK345s2oYC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Create a Topic id/name dictionary \n",
        "\n",
        "When populating your topic id/name dictionary, use the index ordering as shown in the viz tool. \n",
        "\n",
        "We'll use a function to map the the viz tool index ordering with the train LDA model ordering. "
      ],
      "metadata": {
        "id": "9gie-6YrzGeB"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "# create a dictionary \n",
        "# keys - use topic ids from pyLDAvis visualization \n",
        "# values - topic names that you create \n",
        "# save dictionary to `vis_topic_name_dict`\n",
        "\n",
        "vis_topic_name_dict = {}\n",
        "# YOUR CODE HERE\n",
        "raise NotImplementedError()"
      ],
      "outputs": [],
      "metadata": {
        "deletable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "fc10e9728e5dcf06baf0800c3cdb88ce",
          "grade": false,
          "grade_id": "cell-4905c0c1050f0d03",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false
        },
        "id": "88vMiVwQzGeC"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "def get_topic_id_lookup_dict(vis, vis_topic_name_dict):\n",
        "    \"\"\"\n",
        "    Both the starting index and the ordering of topic ids bewteen the trained LDA model \n",
        "    and the viz tool are different. So we need to create a look up dictionary that maps \n",
        "    the correct association between topic ids from both sources. \n",
        "    \"\"\"\n",
        "    # value is order of topic ids accoridng to pyLDAvis tool \n",
        "    # key is order of topic ids according to lda model\n",
        "    model_vis_tool_topic_id_lookup = vis.topic_coordinates.topics.to_dict()\n",
        "\n",
        "    # invert dictionary so that \n",
        "    # key is order of topic ids accoridng to pyLDAvis tool \n",
        "    # value is order of topic ids according to lda model\n",
        "    topic_id_lookup =  {v:k for k, v in model_vis_tool_topic_id_lookup.items()}\n",
        "    \n",
        "    return {v:vis_topic_name_dict[k]  for k, v in topic_id_lookup.items()}"
      ],
      "outputs": [],
      "metadata": {
        "id": "iWLsK27ZzGeC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create a topic id/name look up dict \n",
        "that is aligned with the index ordering of the trained LDA model"
      ],
      "metadata": {
        "id": "tR8xm8IG4M0M"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "topic_name_dict = get_topic_id_lookup_dict(vis, vis_topic_name_dict)"
      ],
      "outputs": [],
      "metadata": {
        "deletable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "55f23aa9324a225091037f0c5daf2192",
          "grade": false,
          "grade_id": "cell-d38acb7b250b4079",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false
        },
        "id": "dcsa0f_tzGeC"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "topic_name_dict"
      ],
      "outputs": [],
      "metadata": {
        "id": "tbeEkAlC4WNi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Use the LDA Model to Assign Each Document a Topic Name\n",
        "\n",
        "Now that we have a topic id/name look up dict that is aligned with the index ordering of the trained LDA model, we can move forward to giving each topic a topic name. \n",
        "\n",
        "The function below has been given to you. However, you highly encouraged to read through it and make sure that you understand what it is doing each step of the way. In fact, a good way to do this is to copy and paste the code inside of the function into a new cell, comment out all the lines of code and line by line, uncomment the code and see the output. "
      ],
      "metadata": {
        "id": "t4NLAlB9zGeC"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "def get_topic_ids_for_docs(lda_model, corpus):\n",
        "    \n",
        "    \"\"\"\n",
        "    Passes a Bag-of-Words vector into a trained LDA model in order to get the topic id of that document. \n",
        "    \n",
        "    Parameters\n",
        "    ----------\n",
        "    lda_model: Gensim object\n",
        "        Must be a trained model \n",
        "        \n",
        "    corpus: nested lists of tuples, \n",
        "        i.e. [[(),(), ..., ()], [(),(), ..., ()], ..., [(),(), ..., ()]]\n",
        "        \n",
        "    Returns\n",
        "    -------\n",
        "    topic_id_list: list\n",
        "        Contains topic ids for all document vectors in corpus \n",
        "    \"\"\"\n",
        "    \n",
        "    # store topic ids for each document\n",
        "    doc_topic_ids = []\n",
        "\n",
        "    # iterature through the bow vectors for each doc\n",
        "    for doc_bow in corpus:\n",
        "        \n",
        "        # store the topic ids for the doc\n",
        "        topic_ids = []\n",
        "        # store the topic probabilities for the doc\n",
        "        topic_probs = []\n",
        "\n",
        "        # list of tuples\n",
        "        # each tuple has a topic id and the prob that the doc belongs to that topic \n",
        "        topic_id_prob_tuples = lda_trained_model.get_document_topics(doc_bow)\n",
        "        \n",
        "        # iterate through the topic id/prob pairs \n",
        "        for topic_id_prob in topic_id_prob_tuples:\n",
        "            \n",
        "            # index for topic id\n",
        "            topic_id = topic_id_prob[0]\n",
        "            # index for prob that doc belongs that the corresponding topic\n",
        "            topic_prob = topic_id_prob[1]\n",
        "\n",
        "            # store all topic ids for doc\n",
        "            topic_ids.append(topic_id)\n",
        "            # store all topic probs for doc\n",
        "            topic_probs.append(topic_prob)\n",
        "\n",
        "        # get the index for the topic that had the highest probability, for the current document \n",
        "        max_topic_prob_ind = np.argmax(topic_probs)\n",
        "        # get the corresponding topic id\n",
        "        max_prob_topic_id = topic_ids[max_topic_prob_ind]\n",
        "        # store the most probable topic id for the current document\n",
        "        doc_topic_ids.append(max_prob_topic_id)\n",
        "        \n",
        "    return doc_topic_ids"
      ],
      "outputs": [],
      "metadata": {
        "id": "23No1SMKzGeC"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "# use get_topic_ids_for_docs to get the topic id for each doc in the corpus - save result to `doc_topic_ids`\n",
        "doc_topic_ids = \n",
        "\n",
        "# create a new feature in df_electronics called topic_id using `doc_topic_ids`\n",
        "df_electronics['topic_id'] = \n",
        "\n",
        "# iterate through topic_id and use the lookup dict `topic_name_dict` to assign each document a topic name\n",
        "# save results to a new feature in df_electronics called `new_topic_name`\n",
        "df_electronics['new_topic_name'] = df_electronics['topic_id'].apply()\n",
        "\n",
        "# YOUR CODE HERE\n",
        "raise NotImplementedError()"
      ],
      "outputs": [],
      "metadata": {
        "deletable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "09f2b8ebaf696dc05187cecce8c9e251",
          "grade": false,
          "grade_id": "cell-e28e8774a79ac647",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false
        },
        "id": "gI94W2MvzGeD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Congratulations! You have created new topic names for your documents. "
      ],
      "metadata": {
        "id": "NZa4oKuIzGeD"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "cols = [\"reviews.text\", \"new_topic_name\", \"topic_id\"]\n",
        "df_electronics[cols].head(15)"
      ],
      "outputs": [],
      "metadata": {
        "id": "kDVAZTa23xkF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "-----\n",
        "# Stretch Goals -- see if you can create a model to classify the reviews into the latent topics you've discovered!\n",
        "\n",
        "\n",
        "- Treat `topic_id` as a y vector and train a supervised learning model to predict the topic of each document\n",
        "- Report your results on the Slack channel!"
      ],
      "metadata": {
        "id": "gkYnBihNzGeE"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [],
      "outputs": [],
      "metadata": {
        "id": "Gc3bhF4E3fcE"
      }
    }
  ]
}