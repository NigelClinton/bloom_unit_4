{
  "nbformat": 4,
  "nbformat_minor": 2,
  "metadata": {
    "colab": {
      "name": "LS_DS_422_Train_Assignment.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "py37  (Python3)",
      "language": "python",
      "name": "py37"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.0"
    },
    "nteract": {
      "version": "0.22.4"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "# Train Practice\n",
        "\n",
        "## *Data Science Unit 4 Sprint 2 Assignment 2*\n",
        "\n",
        "Continue to use TensorFlow Keras & a sample of the [Quickdraw dataset](https://github.com/googlecreativelab/quickdraw-dataset) to build a sketch classification model. The dataset has been sampled to only 10 classes and 10000 observations per class. Please build a baseline classification model then run a few experiments with different optimizers and learning rates. \n",
        "\n",
        "*Don't forget to switch to GPU on Colab!*\n",
        "\n",
        "\n",
        "------\n",
        "\n",
        "# Objective \n",
        "\n",
        "We are going to run a few experiments today\n",
        "\n",
        "- Train a model with and without normalized data and investigate the weight values and learning outcomes\n",
        "- Train a model with varying values for batch_size, learning_rate, and optimizers\n",
        "\n",
        "We are essentially running manual gridsearches on our models. In module 3, we'll learn a few different ways to automate gridseach for deep learning. "
      ],
      "metadata": {
        "id": "NGGrt9EYlCqY"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "import os\r\n",
        "import datetime\r\n",
        "import random\r\n",
        "\r\n",
        "import numpy as np\r\n",
        "import pandas as pd\r\n",
        "import matplotlib.pyplot as plt\r\n",
        "\r\n",
        "import tensorflow as tf\r\n",
        "from tensorflow.keras import Sequential\r\n",
        "from tensorflow.keras.layers import Dense, ReLU\r\n",
        "from sklearn.model_selection import train_test_split\r\n",
        "\r\n",
        "from keras.optimizers import Adam, SGD\r\n",
        "from keras.activations import relu\r\n",
        "from tensorflow.keras.callbacks import TensorBoard"
      ],
      "outputs": [],
      "metadata": {
        "id": "bCd-aeOL_dTw"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "%matplotlib inline\r\n",
        "%load_ext tensorboard"
      ],
      "outputs": [],
      "metadata": {
        "id": "vfqxomHC_dTx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Load data\n",
        "\n",
        "- Don't normalize your data just yet!"
      ],
      "metadata": {
        "id": "bwqzOH5r_dTx"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "def load_quickdraw10():\r\n",
        "    \"\"\"\r\n",
        "    Fill out this doc string, and comment the code, for practice in writing the kind of code that will get you hired. \r\n",
        "    \"\"\"\r\n",
        "    \r\n",
        "    URL_ = \"https://github.com/LambdaSchool/DS-Unit-4-Sprint-2-Neural-Networks/blob/main/quickdraw10.npz?raw=true\"\r\n",
        "    \r\n",
        "    path_to_zip = tf.keras.utils.get_file('./quickdraw10.npz', origin=URL_, extract=False)\r\n",
        "\r\n",
        "    data = np.load(path_to_zip)\r\n",
        "\r\n",
        "    X = data['arr_0']\r\n",
        "    Y = data['arr_1']\r\n",
        "        \r\n",
        "    return train_test_split(X, Y, shuffle=True)"
      ],
      "outputs": [],
      "metadata": {
        "id": "nJsIsrvp7O3e"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "X_train, X_test, y_train, y_test = load_quickdraw10()"
      ],
      "outputs": [],
      "metadata": {
        "id": "9TyCLWEg_dTy"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "n_labels = len(np.unique(y_train))"
      ],
      "outputs": [],
      "metadata": {
        "id": "pMT0yVuO_dTz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "----\n",
        "### Write a Model Function\n",
        "- Write a function called `create_model` which returns a compiled TensorFlow Keras Sequential Model suitable for classifying the QuickDraw-10 dataset. \n",
        "\n",
        "Your function `create_model` should accept the following parameters\n",
        "\n",
        "- Learning Rate `lr`\n",
        "- Optimizer `opt`\n",
        "\n",
        "\n",
        "Build a model with the following architecture and parameter values\n",
        "\n",
        "- Use `1 hidden layer` \n",
        "- Use `sigmoid` activation function in the hidden layer\n",
        "- Use `250 nodes` in the hidden layer \n",
        "- Use `10 nodes` in the output layer\n",
        "- Use `softmax` activation function in the output layer\n",
        "- Use `sparse_categorical_crossentropy` loss function\n",
        "- Use `accuracy` as your metric \n",
        "\n",
        "We will use this function to build all the models that we'll need to run our experiments. "
      ],
      "metadata": {
        "id": "l-6PxI6H5__2"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "def create_model(lr=.01, opt=\"adam\"):\n",
        "    \"\"\"\n",
        "    \n",
        "    Build and returns a complies Keras model.  \n",
        "    \n",
        "    Parameters\n",
        "    ----------\n",
        "    lr: float\n",
        "        Learning rate parameter used for Stocastic Gradient Descent \n",
        "        \n",
        "    opt: string\n",
        "        Name of the optimizer to use\n",
        "        Valid options are \"adam\" and \"sgd\"\n",
        "        \n",
        "    Returns\n",
        "    -------\n",
        "    model: keras object \n",
        "        A compiled keras model \n",
        "    \"\"\"\n",
        "\n",
        "    if opt == \"adam\":\n",
        "        opt = Adam(learning_rate=lr)\n",
        "    elif opt == 'sgd':\n",
        "        opt = SGD(learning_rate=lr)\n",
        "    else:\n",
        "        print (\"{} is not a valid option. Defaulting to Adam optimizer\".format(opt))\n",
        "        opt = Adam(learning_rate=lr)\n",
        "\n",
        "    # build model here\n",
        "# YOUR CODE HERE\n",
        "raise NotImplementedError()"
      ],
      "outputs": [],
      "metadata": {
        "deletable": false,
        "id": "nEREYT-3wI1f",
        "nbgrader": {
          "cell_type": "code",
          "checksum": "6692fdabca8804cd50380fd4b9a56169",
          "grade": false,
          "grade_id": "cell-355125ca910bfedb",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false
        }
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "# a check on model architecture\n",
        "model = create_model()\n",
        "n_layers = len(model.get_config()[\"layers\"])\n",
        "output_act_funct =  model.get_config()[\"layers\"][-1][\"config\"][\"activation\"]\n",
        "\n",
        "assert n_layers == 3, \"You should have an input, one hidden, and an output layer\"\n",
        "assert output_act_funct == \"softmax\", \"Output act funct should be softmax\""
      ],
      "outputs": [],
      "metadata": {
        "id": "v_joQGQ5_dT1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "------\n",
        "\n",
        "# Experiment #1: How Does Normalized Input Data Affect Our Model's Learning Outcome?\n",
        "\n",
        "This experiment will answer the above question by training identical models on a normalized data set and a non-normalized data set.\n",
        "\n",
        "Then we will \n",
        "\n",
        "- Analyze the trained weight values of our model \n",
        "- Plot"
      ],
      "metadata": {
        "id": "NlEy462A_dT1"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "# Clear any logs from previous runs\n",
        "!rm -rf ./logs/"
      ],
      "outputs": [],
      "metadata": {
        "id": "23rAEs_F_dT2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Fit Model on Non-Normalized Data"
      ],
      "metadata": {
        "id": "7UI38zJM_dT2"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "# fit model on non-normalized data\n",
        "\n",
        "now = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
        "logdir = os.path.join(\"logs\", f\"No_Normalization-{now}\")\n",
        "tensorboard = TensorBoard(log_dir=logdir,  histogram_freq=1)\n",
        "\n",
        "model = create_model(lr=.001, opt=\"adam\")\n",
        "\n",
        "model.fit(X_train, y_train, \n",
        "          validation_data=(X_test, y_test),\n",
        "          workers=-2, \n",
        "          epochs=10, \n",
        "          batch_size=32, \n",
        "          verbose=1, \n",
        "          callbacks=[tensorboard])"
      ],
      "outputs": [],
      "metadata": {
        "id": "JUBl9E3g_dT3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---------\n",
        "\n",
        "### Fit Model on Normalized Data"
      ],
      "metadata": {
        "id": "Vce0-9v1_dT3"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "# Normalize your training and test sets \n",
        "# save normalized data to X_train_scaled and X_test_scaled\n",
        "\n",
        "# YOUR CODE HERE\n",
        "raise NotImplementedError()"
      ],
      "outputs": [],
      "metadata": {
        "deletable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "ccc6ed9c1e2c04f77eac26d557236dad",
          "grade": false,
          "grade_id": "cell-2792e5f1fbf02c67",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false
        },
        "id": "QJ5pVdvR_dT4"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "# train model on normalized data\n",
        "\n",
        "now = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
        "logdir = os.path.join(\"logs\", f\"Normalization-{now}\")\n",
        "tensorboard = TensorBoard(log_dir=logdir,  histogram_freq=1)\n",
        "\n",
        "norm_model = create_model(lr=.001, opt=\"adam\")\n",
        "\n",
        "norm_model.fit(X_train_scaled, y_train, \n",
        "          validation_data=(X_test_scaled, y_test),\n",
        "          workers=-2, \n",
        "          epochs=10, \n",
        "          batch_size=32, \n",
        "          verbose=1, \n",
        "          callbacks=[tensorboard])"
      ],
      "outputs": [],
      "metadata": {
        "id": "avnCbCBd_dT4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Each Layer is Labeled\n",
        "\n",
        "Take note of the label for each layer in the network. It is these labels that will help you identify the corresponding bias and weight distributions on tensorboard. \n",
        "\n",
        "Assuming that you've run `create_model` 3 times: once for the model check, once to create `model`, and once to create `norm_model`:\n",
        "\n",
        "The name of the layers for the `model` should be \n",
        "- dense_2\n",
        "- dense_3\n",
        "\n",
        "The name of the layers for `norm_model` should be \n",
        "- dense_4\n",
        "- dense_5\n",
        "\n",
        "\n",
        "If you keep retraining one or both of these models, tensorflow will increment the integer used in the layer names.  But that doesn't matter; notice the layer names so you can find their corresponding bias and weight distributions in tensorboard.\n",
        "\n",
        "**Protip:** If you want to reset the integer incrementation that tensorflow uses, you'll need to restart your notebook's kernal. "
      ],
      "metadata": {
        "id": "OjUq-6q-_dT4"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "model.summary()"
      ],
      "outputs": [],
      "metadata": {
        "id": "GUbdDNr6_dT5"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "norm_model.summary()"
      ],
      "outputs": [],
      "metadata": {
        "id": "pnVYSjfR_dT5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Tensorboard \n",
        "\n",
        "- Run the cell below to launch tensorboard \n",
        "- Click on the `SCALARS` tab to see plots that compare the loss and accuracy between the two models\n",
        "- Click on the `HISTOGRAMS` tab to see the distribution of the learned weights "
      ],
      "metadata": {
        "id": "qGxS5G2g_dT5"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "%tensorboard --logdir logs"
      ],
      "outputs": [],
      "metadata": {
        "id": "qy3hogmR_dT5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Tensorboard \n",
        "\n",
        "Check out the loss and accuracy plots on the `SCALARS` tab. \n",
        "\n",
        "You should see that the accuracy is much higher for the model that was given normalized data; conversely, the loss is much lower for the model that was given normalized data.\n",
        "\n",
        "Whenever training a model, we are adjusting the value of the bias and weights in each layer. For simplicity of analysis, we only trained two layers: a hidden layer and the output layer. \n",
        "\n",
        "Now click on the `HISTOGRAM` tab. \n",
        "\n",
        "You should see both of your model's layer names. \n",
        "\n",
        "### Hidden Layer Distributions\n",
        "\n",
        "Collapse the charts that correspond to the output layer, so only the distributions for the hidden layer's weights and bias are shown.  (i.e. Only expand `dense_2` and `dense_4`). \n",
        "\n",
        "Also, don't be confused by the word 'kernal'; that's just the word that Tensorflow uses instead of weights. So, to be clear, **the kernal distributions are the weight values.** \n",
        "\n",
        "The `bias` distributions are the bias values. \n",
        "\n",
        "You should see 10 distributions stacked next to each other, **one distribution per epoch.**\n",
        "\n",
        "The distribution in the far back corresponds to the weight values at epoch 1 (tensorflow starts the count at 0, as the index for a list). The distribution at the very front corresponds to the weight values at the 10th epoch (tensorflow indexing show 9 instead of 10).\n",
        "\n",
        "Notice how the shape of the distribution changes across epochs? That's because their **values are being updated via Gradient Descent.** \n",
        "\n",
        "The distributions that you see are directly responsible for the validation accuracy of our models. They look different between the two models because one model was given normalized data and one wasn't.  So you can conclude that the weight distributions in `dense_4` produce a higher validation accuracy than those in `dense_2`. \n",
        "\n",
        "Now it's time to analyze those weight values more closely. "
      ],
      "metadata": {
        "id": "tiC3zTpL_dT5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "----------\n",
        "### Analyze Weights in Each Layer"
      ],
      "metadata": {
        "id": "raQkErD1_dT6"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "# get the final bias and weight matrices for model\n",
        "layer = model.get_layer(name=\"dense_2\")\n",
        "bias, weights = layer.get_weights()"
      ],
      "outputs": [],
      "metadata": {
        "id": "xZ2stu7Y_dT6"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "# get the final value bias and weight matrices for norm_moel\n",
        "layer = norm_model.get_layer(name=\"dense_4\")\n",
        "bias_norm, weights_norm = layer.get_weights()"
      ],
      "outputs": [],
      "metadata": {
        "id": "IfndG4C0_dT6"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "# this line of code should not show an error if the number of weights is the same for the hidden layer of both models\n",
        "# this line of code is known as a Unit Test \n",
        "assert weights.shape[0] == weights_norm.shape[0]"
      ],
      "outputs": [],
      "metadata": {
        "id": "AqksPgCI_dT6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Initial Weight Values\n",
        "\n",
        "By default, Keras dense layers randomly initialize the weight values using [**GlorotUniform**](https://keras.io/api/layers/initializers/). \n",
        "\n",
        "The cell below is sampling values from the GlorotUniform distribution. Let's sample from the GlorotUniform distribution and plot it to get a sense of the initial distribution of our weights - before Gradient Descent starts updating their values at training time. "
      ],
      "metadata": {
        "id": "SHNQOv6v_dT6"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "# let's take 250 random samples from the GlorotUniform\n",
        "# because they are random samples their exact values might have been a little different for model and norm_model - but we will assume that they were not statistically different \n",
        "# 250 because that's how many weights are in the hidden layer for both of our models\n",
        "initializer = tf.keras.initializers.GlorotUniform(seed=1234)\n",
        "initial_weight_values = initializer(shape=(1, 250))"
      ],
      "outputs": [],
      "metadata": {
        "id": "NTCOs1KR_dT7"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "plt.figure(figsize=(20,6))\n",
        "plt.title(\"Learned Weights in Hidden Layers\")\n",
        "plt.xlabel(\"Weight Values\")\n",
        "plt.grid()\n",
        "\n",
        "# by setting density=True, we are transforming our plots into probability distributions \n",
        "plt.hist(weights, bins=20, alpha=0.5, label=\"weights from model\", density=True);\n",
        "plt.hist(weights_norm, bins=20, alpha=0.5, label=\"weights from norm_model\", density=True);\n",
        "plt.hist(initial_weight_values, bins=20, alpha=0.5, label=\"initial weight values\", density=True);\n",
        "plt.legend(fontsize=\"x-large\");"
      ],
      "outputs": [],
      "metadata": {
        "id": "UaZksJlE_dT7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Observations\n",
        "\n",
        "Your plot should have 3 distributions\n",
        "\n",
        "- weights from a model trained on non-normalized data\n",
        "- weights from a model trained on normalized data\n",
        "- initial weight values sampled from Glorot Uniform distributions \n",
        "\n",
        "Use the plot to answer the following questions."
      ],
      "metadata": {
        "id": "cBBC7LRo_dT7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Comparing the initial weights with weights_from_model, what was the effect of not using normalized data?**"
      ],
      "metadata": {
        "id": "MoT9JU9Q_dT7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "YOUR ANSWER HERE"
      ],
      "metadata": {
        "deletable": false,
        "nbgrader": {
          "cell_type": "markdown",
          "checksum": "500ff9c3902f635e0a9aafa64e2cc95d",
          "grade": true,
          "grade_id": "cell-4d6c92df9d105c46",
          "locked": false,
          "points": 0,
          "schema_version": 3,
          "solution": true,
          "task": false
        },
        "id": "mp6Z2Xel_dT7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Comparing the initial weights with weights_from_norm_model, what was the effect of using normalized data?**"
      ],
      "metadata": {
        "id": "keIioss9_dT8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "YOUR ANSWER HERE"
      ],
      "metadata": {
        "deletable": false,
        "nbgrader": {
          "cell_type": "markdown",
          "checksum": "23d7012c3d7ce759e5389a0dfee1892a",
          "grade": true,
          "grade_id": "cell-614992ba50bf54c4",
          "locked": false,
          "points": 0,
          "schema_version": 3,
          "solution": true,
          "task": false
        },
        "id": "wELSRVOc_dT8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Using your understanding of how Gradient Descent works, why do you think that the distributions between weights_from_model and weights_from_norm_model look so different?**"
      ],
      "metadata": {
        "id": "5bnRcbkv_dT8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "YOUR ANSWER HERE"
      ],
      "metadata": {
        "deletable": false,
        "nbgrader": {
          "cell_type": "markdown",
          "checksum": "52e82163ac0a8d8b47bc613199650fe2",
          "grade": true,
          "grade_id": "cell-598a597c991950f8",
          "locked": false,
          "points": 0,
          "schema_version": 3,
          "solution": true,
          "task": false
        },
        "id": "Ju3U1PVU_dT8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "----\n",
        "\n",
        "# Additional Experiments\n",
        "\n",
        "The previous experiment demonstrated the importance of normalizing our data to maximize model accuracy. In the next few experiments, we will explore the effect that certain values for Batch Size, Learning Rate, and different Optimizers have on model accuracy. \n",
        "\n",
        "Using our **create_model** model building function, conduct the following experiments. "
      ],
      "metadata": {
        "id": "mopdWDAJ_dT8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Experiment with Batch Size\n",
        "* Run 5 experiments with various batch sizes of your choice. \n",
        "* Visualize the results\n",
        "* Write up an analysis of the experiments and select the \"best\" performing model among your experiments. Make sure to compare against your model's performance yesterday. "
      ],
      "metadata": {
        "id": "f0pCkh8C7eGL"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [],
      "outputs": [],
      "metadata": {
        "id": "USXjs7Hk71Hy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Experiment with Learning Rate\n",
        "* Run 5 experiments with various learning rate magnitudes: 1, .1, .01, .001, .0001.\n",
        "* Use the \"best\" batch size from the previous experiment\n",
        "* Visualize the results\n",
        "* Write up an analysis of the experiments and select the \"best\" performing model among your experiments. Make sure to compare against the previous experiments and your model's performance yesterday. "
      ],
      "metadata": {
        "id": "8b-r70o8p2Dm"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [],
      "outputs": [],
      "metadata": {
        "id": "_SA144xx8Luf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Experiment with Different Optimizers\n",
        "* Run 5 experiments with various optimizers available in TensorFlow. See list [here](https://www.tensorflow.org/api_docs/python/tf/keras/optimizers)\n",
        "* Visualize the results\n",
        "* Write up an analysis of the experiments and select the \"best\" performing model among your experiments. Make sure to compare against the previous experiments and your model's performance yesterday.\n",
        "* Repeat the experiment combining Learning Rate and different optimizers. Does the best-performing model change? "
      ],
      "metadata": {
        "id": "gxMtSRhV9Q7I"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [],
      "outputs": [],
      "metadata": {
        "id": "ujLuzdNA91ip"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "------\n",
        "\n",
        "## Stretch Goals: \n",
        "\n",
        "- On the learning rate experiments, implement [EarlyStopping](https://keras.io/api/callbacks/early_stopping/)\n",
        "- Review the math of Gradient Descent. "
      ],
      "metadata": {
        "id": "FwlRJSfBlCvy"
      }
    }
  ]
}